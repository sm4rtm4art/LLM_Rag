# Testing the LLM RAG System

This directory contains tests for the LLM RAG system, focusing on both unit tests and evaluation tests for the RAG pipeline's response quality.

## Directory Structure

```
tests/
├── test_data/                # Test data for evaluation tests
│   └── test_queries.json     # Sample queries with expected answers
├── test_core.py              # Core functionality tests
├── test_rag_evaluation.py    # RAG response quality evaluation tests
├── test_rag_pipeline.py      # RAG pipeline component tests
└── README.md                 # This file
```

## Test Categories

### Unit Tests

Unit tests verify the correct functioning of individual components:

- `test_core.py`: Tests for core functionality
- `test_rag_pipeline.py`: Tests for RAG pipeline components

### Evaluation Tests

Evaluation tests assess the quality of responses generated by the RAG system:

- `test_rag_evaluation.py`: Tests that evaluate response quality, including:
  - Response relevance to queries
  - Factual accuracy based on expected answers
  - Source document inclusion
  - Handling of uncertainty
  - Response consistency

## Running Tests

### Running All Tests

```bash
pytest
```

### Running Specific Test Files

```bash
pytest tests/test_rag_evaluation.py
```

### Running with Verbosity

```bash
pytest -v tests/test_rag_evaluation.py
```

## Test Data

The `test_data/test_queries.json` file contains sample queries about DIN standards, each with:

- A query string
- Expected answer
- Expected sources

## Pre-commit Integration

The tests are integrated with pre-commit hooks to ensure they run before each commit:

```bash
pre-commit install  # Install the pre-commit hooks
```

This will automatically run the evaluation tests before each commit, ensuring that the RAG system maintains response quality.

## Adding New Tests

### Adding New Evaluation Tests

1. Add new test queries to `test_data/test_queries.json`
2. Create new test functions in `test_rag_evaluation.py`

### Adding New Unit Tests

1. Create new test functions in the appropriate test file
2. Follow the pytest conventions for naming and structuring tests

## Evaluation Metrics

The evaluation tests use the following metrics:

- **Jaccard Similarity**: Measures the similarity between the actual response and the expected answer
- **Source Inclusion**: Checks if the expected sources are included in the response
- **Response Consistency**: Ensures that the same query produces similar responses
