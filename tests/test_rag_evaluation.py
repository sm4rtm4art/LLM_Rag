"""
Test module for evaluating the RAG system responses.

This module provides pytest-based tests to evaluate the quality and accuracy
of responses generated by the RAG pipeline for specific queries.
"""

import json
from pathlib import Path
from typing import Any, Dict, List
from unittest.mock import MagicMock

import pytest

from src.llm_rag.rag.pipeline import RAGPipeline


# Load test queries from JSON file
def load_test_queries():
    """Load test queries from the test data file."""
    test_data_path = Path(__file__).parent / "test_data" / "test_queries.json"

    # Create directory if it doesn't exist
    test_data_path.parent.mkdir(exist_ok=True)

    # Create the file with sample data if it doesn't exist
    if not test_data_path.exists():
        sample_data = [
            {
                "query": "What is DIN VDE 0636-3?",
                "expected_answer": (
                    "DIN VDE 0636-3 is a German standard for low-voltage fuses, "
                    "specifically detailing supplementary requirements for fuses "
                    "used by unskilled persons in household applications."
                ),
                "expected_sources": ["VDE_0636-3_A2_E__DIN_VDE_0636-3_A2__2010-04.pdf"],
            },
            {
                "query": "What are the requirements for fuses according to DIN VDE 0636-3?",
                "expected_answer": (
                    "DIN VDE 0636-3 specifies requirements for fuses used by "
                    "unskilled persons, including safety aspects, standardized "
                    "fuse systems, and national committee responsibilities for "
                    "implementation."
                ),
                "expected_sources": ["VDE_0636-3_A2_E__DIN_VDE_0636-3_A2__2010-04.pdf"],
            },
            {
                "query": "What is the scope of DIN VDE 0636-3?",
                "expected_answer": (
                    "DIN VDE 0636-3 covers standardized fuse systems with respect "
                    "to their safety aspects, with national committees responsible "
                    "for selecting fuse systems from the standard for their "
                    "national standards."
                ),
                "expected_sources": ["VDE_0636-3_A2_E__DIN_VDE_0636-3_A2__2010-04.pdf"],
            },
        ]
        with open(test_data_path, "w") as f:
            json.dump(sample_data, f, indent=2)

    # Load the test data
    with open(test_data_path, "r") as f:
        return json.load(f)


# Mock document for testing
class MockDocument:
    def __init__(self, page_content, metadata):
        self.page_content = page_content
        self.metadata = metadata


# Fixture to provide the RAG pipeline
@pytest.fixture
def rag_pipeline():
    """Fixture to provide an initialized RAG pipeline with mocks."""
    # Create mock vectorstore
    mock_vectorstore = MagicMock()
    mock_document = MockDocument(
        page_content="DIN VDE 0636-3 is a German standard for low-voltage fuses.",
        metadata={"source": "VDE_0636-3_A2_E__DIN_VDE_0636-3_A2__2010-04.pdf"},
    )
    mock_vectorstore.similarity_search.return_value = [mock_document]

    # Create mock LLM
    mock_llm = MagicMock()
    mock_llm.predict.return_value = (
        "DIN VDE 0636-3 is a German standard for low-voltage fuses, "
        "specifically detailing supplementary requirements for fuses "
        "used by unskilled persons in household applications."
    )

    # Create RAG pipeline with mocks
    pipeline = RAGPipeline(vectorstore=mock_vectorstore, llm=mock_llm, top_k=3)

    # Mock the retrieve method to return a list of dictionaries
    pipeline.retrieve = MagicMock(
        return_value=[
            {
                "content": "DIN VDE 0636-3 is a German standard for low-voltage fuses.",
                "metadata": {"source": "VDE_0636-3_A2_E__DIN_VDE_0636-3_A2__2010-04.pdf"},
            }
        ]
    )

    return pipeline


# Fixture to provide test queries
@pytest.fixture
def test_queries():
    """Fixture to provide test queries."""
    return load_test_queries()


def calculate_similarity(text1: str, text2: str) -> float:
    """
    Calculate Jaccard similarity between two texts.

    Args:
        text1: First text
        text2: Second text

    Returns:
        Similarity score between 0 and 1
    """
    # Convert to lowercase and split into words
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    # Calculate Jaccard similarity
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))

    return intersection / union if union > 0 else 0


def check_source_inclusion(sources: List[Dict[str, Any]], expected_sources: List[str]) -> bool:
    """
    Check if the expected sources are included in the actual sources.

    Args:
        sources: List of source documents with metadata
        expected_sources: List of expected source filenames

    Returns:
        True if at least one expected source is found, False otherwise
    """
    if not sources or not expected_sources:
        return False

    for expected in expected_sources:
        for source in sources:
            metadata = source.get("metadata", {})
            source_file = metadata.get("source", "")
            if expected in source_file:
                return True

    return False


@pytest.mark.parametrize("query_index", range(3))
def test_rag_response_quality(rag_pipeline, test_queries, query_index):
    """
    Test the quality of RAG responses for specific queries.

    Args:
        rag_pipeline: The RAG pipeline fixture
        test_queries: The test queries fixture
        query_index: Index of the query to test
    """
    # Skip if query_index is out of range
    if query_index >= len(test_queries):
        pytest.skip(f"No test query at index {query_index}")

    # Get the test case
    test_case = test_queries[query_index]
    query = test_case["query"]
    expected_answer = test_case["expected_answer"]
    expected_sources = test_case.get("expected_sources", [])

    # Configure mock responses based on the query
    if hasattr(rag_pipeline.llm, "predict"):
        rag_pipeline.llm.predict.return_value = expected_answer

    # Run the query
    result = rag_pipeline.query(query)

    # Extract response and source documents
    response = result.get("response", "")
    source_documents = result.get("source_documents", [])

    # Calculate similarity
    similarity = calculate_similarity(response, expected_answer)

    # Check if sources are included
    sources_included = check_source_inclusion(source_documents, expected_sources)

    # Assertions
    assert response, "Response should not be empty"
    assert "DIN VDE 0636" in response, "Response should mention DIN VDE 0636"
    assert len(response) > 50, "Response should have sufficient length"
    assert similarity > 0.3, f"Response similarity ({similarity}) should be above threshold"
    assert sources_included, "Response should include expected sources"


def test_rag_response_uncertainty_handling(rag_pipeline):
    """
    Test how the RAG system handles queries it might not have information about.

    Args:
        rag_pipeline: The RAG pipeline fixture
    """
    # Query that the system might not have information about
    query = "What is the relationship between DIN VDE 0636-3 and ISO 9001:2015?"

    # Configure mock response for uncertainty
    if hasattr(rag_pipeline.llm, "predict"):
        rag_pipeline.llm.predict.return_value = (
            "I don't have specific information about the relationship between "
            "DIN VDE 0636-3 and ISO 9001:2015 in the provided context. "
            "DIN VDE 0636-3 is a German standard for low-voltage fuses, while "
            "ISO 9001:2015 is a quality management system standard."
        )

    # Run the query
    result = rag_pipeline.query(query)
    response = result.get("response", "")

    # Assertions
    assert response, "Response should not be empty"
    assert len(response) > 50, "Response should have sufficient length"

    # The response should either provide information or acknowledge limitations
    uncertainty_phrases = [
        "I don't have specific information",
        "The provided context doesn't mention",
        "I cannot find direct information",
        "There is no explicit mention",
        "The documents do not provide",
    ]

    has_uncertainty = any(phrase in response for phrase in uncertainty_phrases)
    has_content = "DIN VDE 0636-3" in response and "ISO 9001" in response

    assert has_uncertainty or has_content, "Response should either provide information or acknowledge limitations"


def test_rag_response_consistency(rag_pipeline, test_queries):
    """
    Test the consistency of RAG responses for the same query.

    Args:
        rag_pipeline: The RAG pipeline fixture
        test_queries: The test queries fixture
    """
    # Use the first query
    query = test_queries[0]["query"]

    # Configure mock response
    if hasattr(rag_pipeline.llm, "predict"):
        rag_pipeline.llm.predict.return_value = (
            "DIN VDE 0636-3 is a German standard for low-voltage fuses, "
            "specifically detailing supplementary requirements for fuses "
            "used by unskilled persons in household applications."
        )

    # Run the query twice
    result1 = rag_pipeline.query(query)
    result2 = rag_pipeline.query(query)

    response1 = result1.get("response", "")
    response2 = result2.get("response", "")

    # Calculate similarity between the two responses
    similarity = calculate_similarity(response1, response2)

    # Assertions
    assert similarity > 0.8, f"Response consistency ({similarity}) should be high"
