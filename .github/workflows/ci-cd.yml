name: CI/CD Pipeline

# Add permissions at the workflow level
permissions:
  actions: read
  contents: read
  security-events: write # Required for uploading SARIF results

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      deploy_target:
        description: "Environment to deploy to"
        required: false
        default: "dev"
        type: choice
        options:
          - dev
          - staging

jobs:
  # Bash script linting and testing
  bash-lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install shellcheck
        run: |
          sudo apt-get update
          sudo apt-get install -y shellcheck

      - name: Install shfmt
        run: |
          curl -sS https://webinstall.dev/shfmt | bash
          export PATH="$HOME/.local/bin:$PATH"
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Run shellcheck
        run: |
          find scripts -name "*.sh" -type f -exec shellcheck {} \;

      - name: Run shfmt
        run: |
          find scripts -name "*.sh" -type f -exec shfmt -d {} \;

      - name: Run bash script tests
        run: |
          chmod +x scripts/test_bash_scripts.sh
          scripts/test_bash_scripts.sh

  # Security scanning job
  security:
    runs-on: ubuntu-latest
    env:
      UV_LINK_MODE: copy
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      # Add cache before installing UV
      - name: Cache UV packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.cache/pip
          key: ${{ runner.os }}-uv-security-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-security-

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install security tools
        run: uv pip install --system bandit safety

      - name: Run Bandit
        run: |
          bandit -r src/ -c pyproject.toml -f json -o bandit-results.json
        continue-on-error: true

      - name: Upload Bandit results
        uses: actions/upload-artifact@v4
        with:
          name: bandit-results
          path: bandit-results.json

      - name: Run Safety Check
        run: safety check

      # Add cache for Trivy
      - name: Cache Trivy vulnerability database
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: cache-trivy-${{ github.run_id }}
          restore-keys: |
            cache-trivy-

      # Install Trivy directly instead of using the action
      - name: Install Trivy
        run: |
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin v0.48.3
          trivy --version

      # Run Trivy with direct command
      - name: Run Trivy vulnerability scanner
        env:
          TRIVY_NO_PROGRESS: true
          TRIVY_CACHE_DIR: ~/.cache/trivy
          # Don't specify a custom DB repository - use the default
        run: |
          # First update the database explicitly
          trivy --cache-dir ~/.cache/trivy image --download-db-only
          # Then run the scan
          trivy fs --format sarif --output trivy-results.sarif --severity CRITICAL,HIGH .

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: "trivy-results.sarif"

  # Linting job
  lint:
    runs-on: ubuntu-latest
    env:
      UV_LINK_MODE: copy
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      # Add cache before installing UV
      - name: Cache UV packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.cache/pip
            ~/.cache/pre-commit
          key: ${{ runner.os }}-uv-lint-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-lint-

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv pip install --system ruff mypy
          uv pip install --system ".[dev]"

      - name: Run Ruff format check
        run: ruff format "src/" "tests/"

      - name: Run Ruff linting
        run: ruff check "src/" "tests/"

      - name: Run mypy
        run: mypy src tests

  # Testing job
  test:
    runs-on: ubuntu-latest
    env:
      UV_LINK_MODE: copy
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      # Add cache before installing UV
      - name: Cache UV packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.cache/pip
            .pytest_cache
          key: ${{ runner.os }}-uv-test-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-test-

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          uv pip install --system pytest pytest-cov pytest-asyncio
          uv pip install --system sentence-transformers chromadb
          uv pip install --system ".[dev]"

      - name: Run tests with coverage
        run: |
          python -m pytest tests/ \
            --cov=src/llm_rag \
            --cov-report=xml \
            -v

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          slug: sm4rtm4art/llm-rag
          fail_ci_if_error: false
          verbose: true

  # Semantic release job - only runs on main branch pushes
  semantic-release:
    needs: [lint, test]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    permissions:
      contents: write
      issues: write
      pull-requests: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Python Setup
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install python-semantic-release

      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

      - name: Semantic Release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          semantic-release publish

  # Build job - only runs on main branch after all checks pass
  build:
    needs: [security, lint, test, semantic-release, bash-lint]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || github.event_name == 'workflow_dispatch')
    env:
      UV_LINK_MODE: copy
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      # Add cache before installing UV
      - name: Cache UV packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.cache/pip
          key: ${{ runner.os }}-uv-build-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-build-

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      # Check if Docker is available
      - name: Check Docker availability
        id: check-docker
        continue-on-error: true
        run: |
          if docker info &>/dev/null; then
            echo "docker_available=true" >> $GITHUB_OUTPUT
          else
            echo "docker_available=false" >> $GITHUB_OUTPUT
            echo "::warning::Docker is not available. Build and push will be skipped."
          fi

      # Cache Docker layers
      - name: Cache Docker layers
        if: steps.check-docker.outputs.docker_available == 'true'
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Set up Docker Buildx
        if: steps.check-docker.outputs.docker_available == 'true'
        uses: docker/setup-buildx-action@v3
        with:
          buildkitd-flags: --debug

      - name: Login to Docker Hub
        if: steps.check-docker.outputs.docker_available == 'true'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Verify Docker Hub Login
        if: steps.check-docker.outputs.docker_available == 'true'
        run: |
          echo "Checking Docker Hub login..."
          docker login -u ${{ secrets.DOCKERHUB_USERNAME }} --password-stdin <<< ${{ secrets.DOCKERHUB_TOKEN }}
          echo "Checking if repository exists..."
          curl -s -f -L -H "Authorization: Bearer ${{ secrets.DOCKERHUB_TOKEN }}" \
            "https://hub.docker.com/v2/repositories/${{ secrets.DOCKERHUB_USERNAME }}/llm-rag/"

      # Build and push Docker image - explicitly specify linux/amd64 platform only
      - name: Build and push
        if: steps.check-docker.outputs.docker_available == 'true'
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }}
          cache-from: |
            type=local,src=/tmp/.buildx-cache
            type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:buildcache
          cache-to: |
            type=local,dest=/tmp/.buildx-cache-new,mode=max
            type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:buildcache,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1
          # Explicitly specify only linux/amd64 platform to avoid Windows manifest issues
          platforms: linux/amd64
          # Disable provenance attestation which can cause issues with some registries
          provenance: false

      # Move cache to prevent it from growing indefinitely
      - name: Move cache
        if: steps.check-docker.outputs.docker_available == 'true'
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

      - name: Check Docker Hub Repository
        if: steps.check-docker.outputs.docker_available == 'true'
        run: |
          echo "Checking Docker Hub for images..."
          curl -s -H "Authorization: Bearer ${{ secrets.DOCKERHUB_TOKEN }}" \
            "https://hub.docker.com/v2/repositories/${{ secrets.DOCKERHUB_USERNAME }}/llm-rag/tags/" | jq .

      - name: Create build artifact without Docker
        if: steps.check-docker.outputs.docker_available != 'true'
        run: |
          echo "Docker build was skipped. Creating a package instead."
          python -m pip install build
          python -m build
          mkdir -p dist/docker-skipped
          echo "Docker build was skipped on $(date)" > dist/docker-skipped/build-info.txt

      - name: Upload build artifact
        if: steps.check-docker.outputs.docker_available != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: python-package
          path: dist/

  # Development deployment - only runs on main branch
  deploy-dev:
    needs: [build]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name == 'push' || (github.event_name == 'workflow_dispatch' && github.event.inputs.deploy_target == 'dev'))
    # Note: Create 'development' environment in GitHub repository settings before uncommenting
    # environment: development
    steps:
      - uses: actions/checkout@v4

      # Check if Docker image was built
      - name: Check if Docker image exists
        id: check-image
        continue-on-error: true
        run: |
          if curl -s -f -L -H "Authorization: Bearer ${{ secrets.DOCKERHUB_TOKEN }}" \
            "https://hub.docker.com/v2/repositories/${{ secrets.DOCKERHUB_USERNAME }}/llm-rag/tags/${{ github.sha }}" &>/dev/null; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "::warning::Docker image was not built. Deployment will be skipped."
          fi

      # Set up KIND cluster for testing if no Kubernetes config exists
      - name: Check if Kubernetes config exists
        id: check-config
        if: steps.check-image.outputs.image_exists == 'true'
        run: |
          if [ -n "${{ secrets.KUBE_CONFIG_DEV }}" ]; then
            echo "has_config=true" >> $GITHUB_OUTPUT
            echo "use_kind=false" >> $GITHUB_OUTPUT
          else
            echo "has_config=false" >> $GITHUB_OUTPUT
            echo "use_kind=true" >> $GITHUB_OUTPUT
            echo "Using KIND cluster for testing as KUBE_CONFIG_DEV is not set"
          fi

      # Set up KIND cluster if no external Kubernetes config exists
      - name: Set up KIND Cluster
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true'
        uses: engineerd/setup-kind@v0.5.0
        with:
          version: "v0.20.0"
          name: llm-rag-cluster
          config: |
            kind: Cluster
            apiVersion: kind.x-k8s.io/v1alpha4
            nodes:
            - role: control-plane
              kubeadmConfigPatches:
              - |
                kind: InitConfiguration
                nodeRegistration:
                  kubeletExtraArgs:
                    node-labels: "ingress-ready=true"
              extraPortMappings:
              - containerPort: 80
                hostPort: 80
                protocol: TCP
              - containerPort: 443
                hostPort: 443
                protocol: TCP

      # Verify KIND cluster
      - name: Verify KIND Cluster
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true'
        run: |
          kubectl cluster-info
          kubectl get nodes
          # Create namespace for deployment
          kubectl create namespace llm-rag-dev || true

      # Set up kubectl with external config
      - name: Set up kubectl
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.has_config == 'true'
        uses: azure/setup-kubectl@v3

      - name: Configure kubectl
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.has_config == 'true'
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_DEV }}" > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Verify Kubernetes connectivity
        if: steps.check-image.outputs.image_exists == 'true' && (steps.check-config.outputs.has_config == 'true' || steps.check-config.outputs.use_kind == 'true')
        id: verify-k8s
        continue-on-error: true
        run: |
          if kubectl cluster-info; then
            echo "k8s_connected=true" >> $GITHUB_OUTPUT
          else
            echo "k8s_connected=false" >> $GITHUB_OUTPUT
            echo "::warning::Could not connect to Kubernetes cluster. Deployment will be skipped."
          fi

      # Load Docker image into KIND if using KIND
      - name: Load Docker image into KIND
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          # Pull the image from Docker Hub
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }}
          # Load the image into KIND
          kind load docker-image ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }} --name llm-rag-cluster

      # Update deployment configuration
      - name: Update deployment configuration
        if: steps.check-image.outputs.image_exists == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          # Replace image tag and username in deployment file
          sed -i "s|\${DOCKERHUB_USERNAME}/llm-rag:.*|${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }}|g" k8s/deployment.yaml

          # Set environment-specific variables
          sed -i "s|ENVIRONMENT: \".*\"|ENVIRONMENT: \"dev\"|g" k8s/deployment.yaml

          # Apply the configuration
          if [ "${{ steps.check-config.outputs.use_kind }}" == "true" ]; then
            # For KIND, apply to the llm-rag-dev namespace
            kubectl apply -f k8s/deployment.yaml -n llm-rag-dev
            kubectl rollout status deployment/llm-rag -n llm-rag-dev --timeout=120s
          else
            # For external cluster, apply as configured
            kubectl apply -f k8s/deployment.yaml --validate=false
            kubectl rollout status deployment/llm-rag --timeout=120s
          fi

      # Show deployment logs and status for KIND
      - name: Show Deployment Logs
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          echo "=== Deployment logs ==="
          kubectl get all -n llm-rag-dev
          kubectl describe pods -n llm-rag-dev
          kubectl logs -l app=llm-rag -n llm-rag-dev --tail=100 || true

      - name: Show Events
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          echo "=== Events ==="
          kubectl get events -n llm-rag-dev --sort-by=.metadata.creationTimestamp

      - name: Download build artifact
        if: steps.check-image.outputs.image_exists != 'true'
        uses: actions/download-artifact@v4
        with:
          name: python-package
          path: dist/

      - name: Deploy alternative (non-Docker)
        if: steps.check-image.outputs.image_exists != 'true'
        run: |
          echo "Docker image was not available. Deployment skipped."
          echo "Alternative deployment could be implemented here."
          ls -la dist/

  # Staging deployment - only runs when manually triggered with staging option
  deploy-staging:
    needs: [deploy-dev]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'workflow_dispatch' && github.event.inputs.deploy_target == 'staging'
    # Note: Create 'staging' environment in GitHub repository settings before uncommenting
    # environment: staging
    steps:
      - uses: actions/checkout@v4

      # Check if Docker image was built
      - name: Check if Docker image exists
        id: check-image
        continue-on-error: true
        run: |
          if curl -s -f -L -H "Authorization: Bearer ${{ secrets.DOCKERHUB_TOKEN }}" \
            "https://hub.docker.com/v2/repositories/${{ secrets.DOCKERHUB_USERNAME }}/llm-rag/tags/${{ github.sha }}" &>/dev/null; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "::warning::Docker image was not built. Deployment will be skipped."
          fi

      # Set up KIND cluster for testing if no Kubernetes config exists
      - name: Check if Kubernetes config exists
        id: check-config
        if: steps.check-image.outputs.image_exists == 'true'
        run: |
          if [ -n "${{ secrets.KUBE_CONFIG_STAGING }}" ]; then
            echo "has_config=true" >> $GITHUB_OUTPUT
            echo "use_kind=false" >> $GITHUB_OUTPUT
          else
            echo "has_config=false" >> $GITHUB_OUTPUT
            echo "use_kind=true" >> $GITHUB_OUTPUT
            echo "Using KIND cluster for testing as KUBE_CONFIG_STAGING is not set"
          fi

      # Set up KIND cluster if no external Kubernetes config exists
      - name: Set up KIND Cluster
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true'
        uses: engineerd/setup-kind@v0.5.0
        with:
          version: "v0.20.0"
          name: llm-rag-cluster
          config: |
            kind: Cluster
            apiVersion: kind.x-k8s.io/v1alpha4
            nodes:
            - role: control-plane
              kubeadmConfigPatches:
              - |
                kind: InitConfiguration
                nodeRegistration:
                  kubeletExtraArgs:
                    node-labels: "ingress-ready=true"
              extraPortMappings:
              - containerPort: 80
                hostPort: 80
                protocol: TCP
              - containerPort: 443
                hostPort: 443
                protocol: TCP

      # Verify KIND cluster
      - name: Verify KIND Cluster
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true'
        run: |
          kubectl cluster-info
          kubectl get nodes
          # Create namespace for deployment
          kubectl create namespace llm-rag-staging || true

      # Set up kubectl with external config
      - name: Set up kubectl
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.has_config == 'true'
        uses: azure/setup-kubectl@v3

      - name: Configure kubectl
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.has_config == 'true'
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING }}" > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Verify Kubernetes connectivity
        if: steps.check-image.outputs.image_exists == 'true' && (steps.check-config.outputs.has_config == 'true' || steps.check-config.outputs.use_kind == 'true')
        id: verify-k8s
        continue-on-error: true
        run: |
          if kubectl cluster-info; then
            echo "k8s_connected=true" >> $GITHUB_OUTPUT
          else
            echo "k8s_connected=false" >> $GITHUB_OUTPUT
            echo "::warning::Could not connect to Kubernetes cluster. Deployment will be skipped."
          fi

      # Load Docker image into KIND if using KIND
      - name: Load Docker image into KIND
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          # Pull the image from Docker Hub
          docker pull ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }}
          # Load the image into KIND
          kind load docker-image ${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }} --name llm-rag-cluster

      # Update deployment configuration
      - name: Update deployment configuration
        if: steps.check-image.outputs.image_exists == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          # Replace image tag and username in deployment file
          sed -i "s|\${DOCKERHUB_USERNAME}/llm-rag:.*|${{ secrets.DOCKERHUB_USERNAME }}/llm-rag:${{ github.sha }}|g" k8s/deployment.yaml

          # Set environment-specific variables
          sed -i "s|ENVIRONMENT: \".*\"|ENVIRONMENT: \"staging\"|g" k8s/deployment.yaml

          # Apply the configuration
          if [ "${{ steps.check-config.outputs.use_kind }}" == "true" ]; then
            # For KIND, apply to the llm-rag-staging namespace
            kubectl apply -f k8s/deployment.yaml -n llm-rag-staging
            kubectl rollout status deployment/llm-rag -n llm-rag-staging --timeout=120s
          else
            # For external cluster, apply as configured
            kubectl apply -f k8s/deployment.yaml --validate=false
            kubectl rollout status deployment/llm-rag --timeout=120s
          fi

      # Show deployment logs and status for KIND
      - name: Show Deployment Logs
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          echo "=== Deployment logs ==="
          kubectl get all -n llm-rag-staging
          kubectl describe pods -n llm-rag-staging
          kubectl logs -l app=llm-rag -n llm-rag-staging --tail=100 || true

      - name: Show Events
        if: steps.check-image.outputs.image_exists == 'true' && steps.check-config.outputs.use_kind == 'true' && steps.verify-k8s.outputs.k8s_connected == 'true'
        run: |
          echo "=== Events ==="
          kubectl get events -n llm-rag-staging --sort-by=.metadata.creationTimestamp

      - name: Download build artifact
        if: steps.check-image.outputs.image_exists != 'true'
        uses: actions/download-artifact@v4
        with:
          name: python-package
          path: dist/

      - name: Deploy alternative (non-Docker)
        if: steps.check-image.outputs.image_exists != 'true'
        run: |
          echo "Docker image was not available. Deployment skipped."
          echo "Alternative deployment could be implemented here."
          ls -la dist/
