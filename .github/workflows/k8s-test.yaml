name: Kubernetes Test with kind

on:
  workflow_run:
    workflows: ["CI/CD Pipeline"]
    types:
      - completed
    branches: [main, KIND_cluster_running]
  pull_request:
    paths:
      - "k8s/**"
      - "Dockerfile"
      - "Dockerfile.k8s-test"
      - ".github/workflows/k8s-test.yaml"
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: "Enable verbose debug logs"
        required: false
        default: false
        type: boolean

env:
  CLUSTER_NAME: llm-rag-test
  NAMESPACE: llm-rag-test
  IMAGE_NAME: llm-rag
  KIND_VERSION: v0.20.0
  KUBECTL_VERSION: v1.28.2
  USE_TEST_IMAGE: true # Flag to use the lightweight test image
  KIND_KUBECONFIG: /tmp/kind-kubeconfig

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.set-image-tag.outputs.image_tag }}
      cache_hit: ${{ steps.restore-kind-cache.outputs.cache-hit }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      # Determine image tag based on trigger
      - name: Set image tag
        id: set-image-tag
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] || [ "${{ github.event_name }}" == "pull_request" ]; then
            # For manual runs or PRs, use the current SHA
            echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" == "workflow_run" ]; then
            # For workflow_run triggers, use the SHA from the triggering workflow if available
            if [ -n "${{ github.event.workflow_run.head_sha }}" ]; then
              echo "image_tag=${{ github.event.workflow_run.head_sha }}" >> $GITHUB_OUTPUT
            else
              echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT
            fi
          else
            # Default fallback
            echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT
          fi

      # Optimize disk space
      - name: Aggressive cleanup before Docker build
        run: |
          echo "Disk before:"
          df -h
          sudo rm -rf /usr/share/dotnet /opt/ghc /opt/hostedtoolcache /usr/local/lib/android || true
          docker system prune -af || true
          docker builder prune -af || true
          docker volume prune -f || true
          echo "Disk after:"
          df -h

      # Cache KIND binary to avoid redownloads
      - name: Cache KIND binary
        id: restore-kind-cache
        uses: actions/cache@v4
        with:
          path: /usr/local/bin/kind
          key: kind-${{ env.KIND_VERSION }}-${{ runner.os }}

  build-or-pull:
    needs: prepare
    runs-on: ubuntu-latest
    outputs:
      image_exists: ${{ steps.check-image-existence.outputs.image_exists }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      # Check if image exists in Docker Hub
      - name: Check if image exists in registry
        id: check-image-existence
        if: github.event_name != 'pull_request'
        env:
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          IMAGE_TAG: ${{ needs.prepare.outputs.image_tag }}
        run: |
          # Check if image exists in Docker Hub without pulling it
          TOKEN=$(curl -s -H "Content-Type: application/json" -X POST \
            -d '{"username": "'${DOCKERHUB_USERNAME}'", "password": "'${{ secrets.DOCKERHUB_TOKEN }}'"}' \
            https://hub.docker.com/v2/users/login/ | jq -r .token)

          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: JWT ${TOKEN}" \
            https://hub.docker.com/v2/repositories/${DOCKERHUB_USERNAME}/${IMAGE_NAME}/tags/${IMAGE_TAG}/)

          if [ "${HTTP_CODE}" == "200" ]; then
            echo "image_exists=true" >> $GITHUB_OUTPUT
            echo "Image ${DOCKERHUB_USERNAME}/${IMAGE_NAME}:${IMAGE_TAG} exists in Docker Hub"
          else
            echo "image_exists=false" >> $GITHUB_OUTPUT
            echo "Image ${DOCKERHUB_USERNAME}/${IMAGE_NAME}:${IMAGE_TAG} does not exist in Docker Hub"
          fi

  pull-image:
    needs: [prepare, build-or-pull]
    runs-on: ubuntu-latest
    if: needs.build-or-pull.outputs.image_exists == 'true' && github.event_name != 'pull_request'
    steps:
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Pull Docker image
        env:
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          IMAGE_TAG: ${{ needs.prepare.outputs.image_tag }}
        run: |
          docker pull ${DOCKERHUB_USERNAME}/${IMAGE_NAME}:${IMAGE_TAG}
          # Tag as latest for KIND
          docker tag ${DOCKERHUB_USERNAME}/${IMAGE_NAME}:${IMAGE_TAG} ${IMAGE_NAME}:latest
          # Save image for next job
          mkdir -p /tmp/images
          docker save ${IMAGE_NAME}:latest -o /tmp/images/image.tar

      - name: Upload image artifact
        uses: actions/upload-artifact@v6
        with:
          name: docker-image
          path: /tmp/images/image.tar
          retention-days: 1

  build-image:
    needs: [prepare, build-or-pull]
    runs-on: ubuntu-latest
    if: needs.build-or-pull.outputs.image_exists != 'true' || github.event_name == 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      - name: Aggressive cleanup before Docker build
        run: |
          echo "Disk before:"
          df -h
          sudo rm -rf /usr/share/dotnet /opt/ghc /opt/hostedtoolcache /usr/local/lib/android || true
          docker system prune -af || true
          docker builder prune -af || true
          docker volume prune -f || true
          echo "Disk after:"
          df -h

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Build Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ${{ env.USE_TEST_IMAGE == 'true' && 'Dockerfile.k8s-test' || 'Dockerfile' }}
          push: false
          load: true
          tags: ${{ env.IMAGE_NAME }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Save image for KIND
        run: |
          mkdir -p /tmp/images
          docker save ${IMAGE_NAME}:latest -o /tmp/images/image.tar

      - name: Upload image artifact
        uses: actions/upload-artifact@v6
        with:
          name: docker-image
          path: /tmp/images/image.tar
          retention-days: 1

  test-kubernetes:
    runs-on: ubuntu-latest
    # Run after either pull-image or build-image job
    needs: [prepare, pull-image, build-image]
    if: |
      always() &&
      (needs.pull-image.result == 'success' || needs.build-image.result == 'success')
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      # Extra aggressive cleanup to ensure enough disk space and memory
      - name: Super aggressive cleanup for KIND
        run: |
          echo "==== Before cleanup ===="
          df -h
          free -h

          # Stop and remove all running containers
          docker rm -f $(docker ps -aq) || true

          # Remove all Docker data
          docker system prune -af --volumes

          # Clear swap
          sudo swapoff -a
          sudo swapon -a

          # Clear page cache and other kernel caches
          sudo sync
          echo 3 | sudo tee /proc/sys/vm/drop_caches

          # Remove large directories
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache || true

          # Remove unnecessary packages (be careful with this)
          sudo apt-get clean
          sudo apt-get autoremove -y

          echo "==== After cleanup ===="
          df -h
          free -h

          # Create tmp directory for KIND logs
          mkdir -p /tmp/kind-logs

      # Download cached image
      - name: Download image artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp/images

      - name: Load Docker image
        run: |
          docker load -i /tmp/images/image.tar
          docker image ls

      # Install KIND with caching
      - name: Install KIND
        if: needs.prepare.outputs.cache_hit != 'true'
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/${{ env.KIND_VERSION }}/kind-linux-amd64
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind

      # Install kubectl
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      # Create cluster with minimal config and explicit kubeconfig
      - name: Create kind cluster
        run: |
          # Create cluster with explicit kubeconfig path and minimal config
          kind create cluster --name ${{ env.CLUSTER_NAME }} --config ./k8s/kind-ci-minimal.yaml --wait 300s --kubeconfig=${KIND_KUBECONFIG}

          # Set KUBECONFIG for all subsequent kubectl commands
          echo "KUBECONFIG=${KIND_KUBECONFIG}" >> $GITHUB_ENV

          # Verify connection to cluster
          kubectl cluster-info
          kubectl get nodes

      # Load image into KIND more efficiently
      - name: Load Docker image into kind
        run: |
          kind load docker-image ${{ env.IMAGE_NAME }}:latest --name ${{ env.CLUSTER_NAME }}

      # Create namespace
      - name: Create namespace
        run: |
          kubectl create namespace ${{ env.NAMESPACE }}

      # Deploy application with configurable timeouts
      - name: Deploy to Kubernetes
        run: |
          # Create a simple pod directly instead of full deployment
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: llm-rag-pod
            namespace: ${{ env.NAMESPACE }}
            labels:
              app: llm-rag
          spec:
            containers:
            - name: llm-rag
              image: ${{ env.IMAGE_NAME }}:latest
              imagePullPolicy: Never
              ports:
              - containerPort: 8000
                name: http
          EOF

          # Create service
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Service
          metadata:
            name: llm-rag
            namespace: ${{ env.NAMESPACE }}
          spec:
            selector:
              app: llm-rag
            ports:
            - port: 80
              targetPort: 8000
              protocol: TCP
            type: ClusterIP
          EOF

          # Wait for pod to be ready
          kubectl wait --for=condition=ready pod/llm-rag-pod -n ${{ env.NAMESPACE }} --timeout=180s

          # Show pod status
          kubectl get pods -n ${{ env.NAMESPACE }}

      # Run test with a dedicated test job
      - name: Create test job
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: llm-rag-test
            namespace: ${{ env.NAMESPACE }}
          spec:
            ttlSecondsAfterFinished: 100
            template:
              spec:
                containers:
                - name: test
                  image: curlimages/curl:8.1.2
                  command: ["/bin/sh", "-c"]
                  args:
                  - |
                    cd /tests
                    chmod +x ./test-script.sh
                    ./test-script.sh
                  volumeMounts:
                  - name: test-scripts
                    mountPath: /tests
                volumes:
                - name: test-scripts
                  configMap:
                    name: test-scripts
                    defaultMode: 0777
                restartPolicy: Never
            backoffLimit: 0
          EOF

          # Create ConfigMap with test script
          kubectl create configmap test-scripts -n ${{ env.NAMESPACE }} --from-file=test-script.sh=k8s/test-script.sh

      # Wait for test job to complete and collect logs
      - name: Wait for test job and collect results
        id: test-results
        run: |
          echo "Waiting for test job to complete..."
          kubectl wait --for=condition=complete --timeout=120s job/llm-rag-test -n ${{ env.NAMESPACE }} || true

          # Collect logs into file
          mkdir -p test-artifacts
          kubectl logs -n ${{ env.NAMESPACE }} job/llm-rag-test > test-artifacts/test-logs.txt || true

          # Check if job succeeded
          if kubectl get job llm-rag-test -n ${{ env.NAMESPACE }} -o jsonpath='{.status.succeeded}' | grep -q "1"; then
            echo "✅ Test job completed successfully"
            echo "Test logs:"
            cat test-artifacts/test-logs.txt
            echo "test_success=true" >> $GITHUB_OUTPUT
          else
            echo "❌ Test job failed"
            echo "Test logs:"
            cat test-artifacts/test-logs.txt
            echo "test_success=false" >> $GITHUB_OUTPUT
            # Don't exit with error here, we'll collect diagnostics first
          fi

      # Collect detailed diagnostics on failure - always run but don't fail the workflow
      - name: Collect diagnostics on failure
        if: steps.test-results.outputs.test_success != 'true'
        continue-on-error: true
        run: |
          echo "Collecting cluster diagnostics..."
          mkdir -p test-artifacts/diagnostics

          # Verify KIND cluster is still running
          if ! kind get clusters | grep -q "${{ env.CLUSTER_NAME }}"; then
            echo "KIND cluster is no longer running, cannot collect diagnostics"
            exit 0
          fi

          # Check KUBECONFIG exists
          if [ ! -f "${KUBECONFIG}" ]; then
            echo "KUBECONFIG file not found at ${KUBECONFIG}"
            exit 0
          fi

          # Verify cluster is reachable
          if ! kubectl cluster-info > /dev/null 2>&1; then
            echo "Kubernetes cluster is not reachable, cannot collect diagnostics"
            exit 0
          fi

          # Collect pod details and logs
          kubectl get all -n ${{ env.NAMESPACE }} -o wide > test-artifacts/diagnostics/all-resources.txt || true
          kubectl describe pods -n ${{ env.NAMESPACE }} > test-artifacts/diagnostics/pod-details.txt || true
          kubectl describe svc -n ${{ env.NAMESPACE }} > test-artifacts/diagnostics/service-details.txt || true

          # Collect container logs
          kubectl logs -l app=llm-rag -n ${{ env.NAMESPACE }} --tail=100 > test-artifacts/diagnostics/app-logs.txt || true

          # Collect events
          kubectl get events -n ${{ env.NAMESPACE }} --sort-by=.metadata.creationTimestamp > test-artifacts/diagnostics/events.txt || true

          # Collect node status
          kubectl describe nodes > test-artifacts/diagnostics/nodes.txt || true

      # Check test results and fail if needed
      - name: Check test results
        if: always() && steps.test-results.outputs.test_success == 'false'
        run: |
          echo "Tests failed, see logs for details"
          exit 1

      # Upload test logs and diagnostics as artifacts
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-artifacts
          path: test-artifacts/
          retention-days: 7

      # Clean up
      - name: Clean up
        if: always()
        run: |
          kind delete cluster --name ${{ env.CLUSTER_NAME }} || true
