#!/usr/bin/env python
"""Real-world test for the refactored RAG pipeline.

This script tests the RAG pipeline with real components:
- A real Chroma vector database with sample documents
- A real LLM (using OpenAI or a local model)
- The modular RAG pipeline components
"""

import logging
import os
import sys
from typing import Any, Dict, List

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import our test data loader
try:
    from load_test_data import SAMPLE_TEXTS, create_documents, load_documents_to_chroma, split_documents
except ImportError:
    logger.error('Could not import load_test_data module')
    sys.exit(1)

# Import RAG pipeline components
try:
    # Import directly from the refactored modules
    from llm_rag.rag.pipeline.base import RAGPipeline
    from llm_rag.rag.pipeline.context import create_formatter
    from llm_rag.rag.pipeline.conversational import ConversationalRAGPipeline
    from llm_rag.rag.pipeline.generation import create_generator

    # We'll also directly import the factory functions for testing
    from llm_rag.rag.pipeline.retrieval import create_retriever
except ImportError as e:
    logger.error(f'Could not import RAG pipeline modules: {e}')
    sys.exit(1)

try:
    # Try to import LangChain components
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_core.documents import Document

    # For OpenAI we use their client directly:
    from langchain_openai import ChatOpenAI
except ImportError as e:
    logger.error(f'Could not import required dependencies: {e}')
    sys.exit(1)


def setup_vectorstore(db_dir: str = './test_chroma_db') -> Chroma:
    """Set up the vector database with test documents."""
    # Check if the database already exists
    if os.path.exists(db_dir) and os.listdir(db_dir):
        logger.info(f'Using existing vector database at {db_dir}')
        # Initialize embedding model
        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
        # Load existing database
        return Chroma(persist_directory=db_dir, embedding_function=embeddings)

    # Create a new database
    logger.info('Creating new vector database with sample documents')
    documents = create_documents(SAMPLE_TEXTS)
    split_docs = split_documents(documents)

    return load_documents_to_chroma(
        documents=split_docs, persist_directory=db_dir, embedding_model_name='all-MiniLM-L6-v2'
    )


def setup_llm() -> Any:
    """Set up the language model."""
    # Check if OpenAI API key is available
    api_key = os.environ.get('OPENAI_API_KEY')
    if api_key:
        logger.info('Using OpenAI model')
        return ChatOpenAI(
            model='gpt-3.5-turbo',
            temperature=0.0,  # Use deterministic responses for testing
            api_key=api_key,
        )

    # If no OpenAI API key, try to set up a local model
    try:
        from langchain_community.llms import LlamaCpp

        # Check if model file exists
        model_path = './models/llama-2-7b-chat.Q4_K_M.gguf'
        if not os.path.exists(model_path):
            raise FileNotFoundError(f'Local model not found at {model_path}')

        logger.info('Using local Llama model')
        return LlamaCpp(
            model_path=model_path,
            temperature=0.1,
            max_tokens=2000,
            verbose=False,
        )
    except (ImportError, FileNotFoundError) as e:
        logger.warning(f'No real LLM available: {e}')
        logger.info('Using a mock LLM for testing')

        # Create a simple mock LLM
        from unittest.mock import MagicMock

        mock_llm = MagicMock()
        mock_llm.invoke.return_value.content = (
            'This is a mock response. In a real scenario, this would be '
            'generated by a language model based on the retrieved context.'
        )
        return mock_llm


def convert_documents_for_formatter(docs: List[Any]) -> List[Dict[str, Any]]:
    """Convert Document objects to dictionaries for formatter compatibility."""
    converted_docs = []

    for doc in docs:
        if isinstance(doc, Document):
            # Convert a LangChain Document to a dictionary
            converted_docs.append({'content': doc.page_content, 'metadata': doc.metadata})
        else:
            # Keep dictionaries as they are
            converted_docs.append(doc)

    return converted_docs


def test_rag_components(vectorstore: Chroma, llm: Any) -> None:
    """Test each RAG component directly."""
    logger.info('Testing individual RAG components')

    # Create components directly using factory functions
    retriever = create_retriever(source=vectorstore, top_k=3)
    formatter = create_formatter(format_type='simple', include_metadata=True)
    generator = create_generator(
        llm=llm,
        prompt_template=(
            'You are a helpful AI assistant. Answer the question based on '
            "the provided context. If you don't know the answer or it's not "
            "in the context, say 'I don't have that information.'\n\n"
            'Context:\n{context}\n\n'
            'Question: {query}\n\n'
            'Answer:'
        ),
    )

    # Test queries
    test_queries = [
        'What is RAG?',
        'How do anti-hallucination techniques work?',
        'What are vector databases used for?',
        'What are DIN standards?',
        'What is quantum computing?',  # Not in our dataset
    ]

    for i, query in enumerate(test_queries):
        logger.info(f'\n\nTesting query {i + 1}: {query}')

        # Step 1: Retrieve relevant documents
        documents = retriever.retrieve(query)
        print(f'\nQuery: {query}')
        print(f'\nRetrieved {len(documents)} documents:')
        for j, doc in enumerate(documents):
            # Handle Document objects from LangChain
            if isinstance(doc, Document):
                metadata = doc.metadata
                content = doc.page_content
                title = metadata.get('title', 'Unknown')
            else:
                # Fall back to dictionary style access
                metadata = doc.get('metadata', {})
                content = doc.get('content', '')
                title = metadata.get('title', 'Unknown')

            print(f'  Document {j + 1}: {title}')
            if len(content) > 100:
                print(f'  Content: {content[:100]}...')
            else:
                print(f'  Content: {content}')

        # Step 2: Convert documents for formatter
        converted_docs = convert_documents_for_formatter(documents)

        # Step 3: Format the context
        context = formatter.format_context(converted_docs)
        print(f'\nFormatted context length: {len(context)} characters')

        # Step 4: Generate a response
        response = generator.generate(query=query, context=context)
        print(f'\nGenerated response: {response}')


def test_rag_pipeline(vectorstore: Chroma, llm: Any) -> None:
    """Test the RAG pipeline using internal components."""
    logger.info('Creating RAG pipeline with real components')

    # Create a RAGPipeline instance
    pipeline = RAGPipeline(
        vectorstore=vectorstore,
        llm=llm,
        top_k=3,
        prompt_template=(
            'You are a helpful AI assistant. Answer the question based on '
            "the provided context. If you don't know the answer or it's not "
            "in the context, say 'I don't have that information.'\n\n"
            'Context:\n{context}\n\n'
            'Question: {query}\n\n'
            'Answer:'
        ),
    )

    # The components are available as attributes
    retriever = pipeline._retriever
    formatter = pipeline._formatter
    generator = pipeline._generator

    # Test queries
    test_queries = [
        'What is RAG?',
        'How do anti-hallucination techniques work?',
        'What are vector databases used for?',
        'What are DIN standards?',
        'What is quantum computing?',  # Not in our dataset
    ]

    for i, query in enumerate(test_queries):
        logger.info(f'\n\nTesting query {i + 1}: {query}')

        # Step 1: Retrieve relevant documents
        documents = retriever.retrieve(query)
        print(f'\nQuery: {query}')
        print(f'\nRetrieved {len(documents)} documents:')
        for j, doc in enumerate(documents):
            # Handle Document objects from LangChain
            if isinstance(doc, Document):
                metadata = doc.metadata
                content = doc.page_content
                title = metadata.get('title', 'Unknown')
            else:
                # Fall back to dictionary style access
                metadata = doc.get('metadata', {})
                content = doc.get('content', '')
                title = metadata.get('title', 'Unknown')

            print(f'  Document {j + 1}: {title}')
            if len(content) > 100:
                print(f'  Content: {content[:100]}...')
            else:
                print(f'  Content: {content}')

        # Step 2: Convert documents for formatter
        converted_docs = convert_documents_for_formatter(documents)

        # Step 3: Format the context
        context = formatter.format_context(converted_docs)
        print(f'\nFormatted context length: {len(context)} characters')

        # Step 4: Generate a response
        response = generator.generate(query=query, context=context)
        print(f'\nGenerated response: {response}')


def test_conversational_pipeline(vectorstore: Chroma, llm: Any) -> None:
    """Test the conversational RAG pipeline."""
    logger.info('Creating conversational RAG pipeline with real components')

    # Create a ConversationalRAGPipeline instance
    pipeline = ConversationalRAGPipeline(vectorstore=vectorstore, llm=llm, top_k=3, history_size=3)

    # Access the internal components
    retriever = pipeline._retriever
    formatter = pipeline._formatter
    generator = pipeline._generator

    # Test conversation
    conversation = [
        'What is RAG?',
        'What are the main components?',
        'How does it help with hallucination?',
        'Are there any alternatives to vector databases?',
    ]

    conversation_id = 'test-conversation-1'

    # Reset history to start clean
    if hasattr(pipeline, 'reset_history'):
        pipeline.reset_history(conversation_id)

    # Initialize empty history
    history = ''

    for i, query in enumerate(conversation):
        logger.info(f'\n\nConversation turn {i + 1}: {query}')

        # Step 1: Retrieve relevant documents
        documents = retriever.retrieve(query)
        print(f'\nRetrieved {len(documents)} documents')

        # Step 2: Convert documents for formatter
        converted_docs = convert_documents_for_formatter(documents)

        # Step 3: Format the context
        context = formatter.format_context(converted_docs)

        # Step 4: Generate a response with history
        response = generator.generate(query=query, context=context, history=history)
        print(f'\nUser: {query}')
        print(f'Assistant: {response}')

        # Step 5: Update the history (manually since we're using components directly)
        if hasattr(pipeline, 'add_to_history'):
            pipeline.add_to_history(conversation_id, response, query)
            history = pipeline.format_history(conversation_id)

            # Print the conversation history
            history_turns = len(history.split('Human:')) - 1 if history else 0
            print(f'\nCurrent conversation history ({history_turns} turns):')
            print(history)
        else:
            # Simple history update if methods not available
            history += f'Human: {query}\nAI: {response}\n\n'
            print('\nCurrent conversation history:')
            print(history)


def main():
    """Test the RAG pipeline with real-world data."""
    try:
        # Set up the vector database
        vectorstore = setup_vectorstore()

        # Set up the language model
        llm = setup_llm()

        # Test individual RAG components
        test_rag_components(vectorstore, llm)

        # Test the pipeline using internal components
        test_rag_pipeline(vectorstore, llm)

        # Test the conversational pipeline
        test_conversational_pipeline(vectorstore, llm)

        print('\nTests completed successfully!')

    except Exception as e:
        logger.error(f'Error during testing: {e}', exc_info=True)
        print(f'Error: {e}')
        return 1

    return 0


if __name__ == '__main__':
    sys.exit(main())
